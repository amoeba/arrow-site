<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2025-02-19T19:12:13-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow 19.0.1 Release</title><link href="https://arrow.apache.org/blog/2025/02/16/19.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 19.0.1 Release" /><published>2025-02-16T00:00:00-05:00</published><updated>2025-02-16T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/02/16/19.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/02/16/19.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 19.0.1 release.</p>

<p>This release primarily addresses a bug in the recent Arrow 19.0.0 release which
prevents Arrow C++ and libraries binding it (e.g., Python, R) from reading
Parquet files created by Arrow Rust v53.0.0 or more recent. See the <a href="/blog/2025/01/16/19.0.0-release/">19.0.0
release blog post</a> for more information.</p>

<p>This release includes <a href="https://github.com/apache/arrow/milestone/68?closed=1"><strong>12 resolved issues</strong></a> from <a href="/release/19.0.1.html#contributors"><strong>9 distinct
contributors</strong></a>. See the <a href="/install">Install Page</a> to learn how to get the libraries
for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Other bugfixes and improvements have been made: we refer you to
the <a href="/release/19.0.1.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>A bug has been fixed which caused reading of Parquet files written by Arrow
Rust v53.0.0 or higher to always fail
<a href="https://github.com/apache/arrow/issues/45283">(#45283)</a>.</li>
  <li>A workaround has been added to increase compatibility with newer versions of
the AWS C++ SDK (<a href="https://github.com/apache/arrow/issues/45304">#45304</a>).</li>
  <li>An overflow bug has been fixed which could cause segmentation faults when
joining inputs over a certain size
(<a href="https://github.com/apache/arrow/issues/44513">#44513</a>).</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>A bug has been fixed on the integration with Pandas when using
<code class="language-plaintext highlighter-rouge">future.infer_string=True</code> for Pandas 2.2
(<a href="https://github.com/apache/arrow/issues/45296">#45296</a>).</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 19.0.1 release. This release primarily addresses a bug in the recent Arrow 19.0.0 release which prevents Arrow C++ and libraries binding it (e.g., Python, R) from reading Parquet files created by Arrow Rust v53.0.0 or more recent. See the 19.0.0 release blog post for more information. This release includes 12 resolved issues from 9 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes A bug has been fixed which caused reading of Parquet files written by Arrow Rust v53.0.0 or higher to always fail (#45283). A workaround has been added to increase compatibility with newer versions of the AWS C++ SDK (#45304). An overflow bug has been fixed which could cause segmentation faults when joining inputs over a certain size (#44513). Python notes A bug has been fixed on the integration with Pandas when using future.infer_string=True for Pandas 2.2 (#45296).]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 16 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2025/01/21/adbc-16-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 16 (Libraries) Release" /><published>2025-01-21T00:00:00-05:00</published><updated>2025-01-21T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/21/adbc-16-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/21/adbc-16-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the version 16 release of
the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/20"><strong>15
resolved issues</strong></a> from <a href="#contributors"><strong>11 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version 16.  The
<a href="https://arrow.apache.org/adbc/16/format/specification.html"><strong>API specification</strong></a> is versioned separately and is at
version 1.1.0.</p>

<p>The subcomponents are versioned independently:</p>

<ul>
  <li>C/C++/GLib/Go/Python/Ruby: 1.4.0</li>
  <li>C#: 0.16.0</li>
  <li>Java: 0.16.0</li>
  <li>R: 0.16.0</li>
  <li>Rust: 0.16.0</li>
</ul>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-16/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This release focused mostly on bugfixes.</p>

<p>The C# ADO.NET adapter can now parse connection properties from the connection string (<a href="https://github.com/apache/arrow-adbc/pull/2352">#2352</a>).  The driver for various Thrift-based systems (Hive/Impala/Spark) now supports timeout options (<a href="https://github.com/apache/arrow-adbc/pull/2312">#2312</a>).  A package was added to wrap the Arrow Flight SQL driver (written in Go) from C# (<a href="https://github.com/apache/arrow-adbc/pull/2214">#2214</a>).</p>

<p>The PostgreSQL driver was fixed to properly return unknown types as <code class="language-plaintext highlighter-rouge">arrow.opaque</code> again (<a href="https://github.com/apache/arrow-adbc/pull/2450">#2450</a>) and to avoid issuing an unnecessary <code class="language-plaintext highlighter-rouge">COMMIT</code> which would cause the driver and connection state to get out of sync (<a href="https://github.com/apache/arrow-adbc/pull/2412">#2412</a>).</p>

<p>Python packages only require manylinux2014 again; the baseline glibc requirement was unintentionally raised in the last release and has now been reverted (<a href="https://github.com/apache/arrow-adbc/issues/2350">#2350</a>).</p>

<p>A breaking change was made in the unstable Rust APIs to return a <code class="language-plaintext highlighter-rouge">Result</code> from a fallible function (<a href="https://github.com/apache/arrow-adbc/pull/2334">#2334</a>).  An <code class="language-plaintext highlighter-rouge">adbc_snowflake</code> crate was added to wrap the Snowflake driver (written in Go) into the Rust APIs, though it is not yet being published (<a href="https://github.com/apache/arrow-adbc/pull/2207">#2207</a>).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-15..apache-arrow-adbc-16
    23	David Li
     8	Matthijs Brobbel
     4	davidhcoe
     3	Bruce Irschick
     2	Matt Topol
     1	Albert LI
     1	Cocoa
     1	Curt Hagenlocher
     1	Jacob Wujciak-Jens
     1	Julian Brandrick
     1	qifanzhang-ms
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support.  For more, see the <a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>; the proposed C Data Interface extensions have been accepted.</p>

<p>We would welcome comments on APIs that could be added or extended, for instance see <a href="https://github.com/apache/arrow-adbc/issues/1704">#1704</a>.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 16 release of the Apache Arrow ADBC libraries. This release includes 15 resolved issues from 11 distinct contributors. This is a release of the libraries, which are at version 16. The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.4.0 C#: 0.16.0 Java: 0.16.0 R: 0.16.0 Rust: 0.16.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This release focused mostly on bugfixes. The C# ADO.NET adapter can now parse connection properties from the connection string (#2352). The driver for various Thrift-based systems (Hive/Impala/Spark) now supports timeout options (#2312). A package was added to wrap the Arrow Flight SQL driver (written in Go) from C# (#2214). The PostgreSQL driver was fixed to properly return unknown types as arrow.opaque again (#2450) and to avoid issuing an unnecessary COMMIT which would cause the driver and connection state to get out of sync (#2412). Python packages only require manylinux2014 again; the baseline glibc requirement was unintentionally raised in the last release and has now been reverted (#2350). A breaking change was made in the unstable Rust APIs to return a Result from a fallible function (#2334). An adbc_snowflake crate was added to wrap the Snowflake driver (written in Go) into the Rust APIs, though it is not yet being published (#2207). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-15..apache-arrow-adbc-16 23 David Li 8 Matthijs Brobbel 4 davidhcoe 3 Bruce Irschick 2 Matt Topol 1 Albert LI 1 Cocoa 1 Curt Hagenlocher 1 Jacob Wujciak-Jens 1 Julian Brandrick 1 qifanzhang-ms Roadmap There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone; the proposed C Data Interface extensions have been accepted. We would welcome comments on APIs that could be added or extended, for instance see #1704. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 19.0.0 Release</title><link href="https://arrow.apache.org/blog/2025/01/16/19.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 19.0.0 Release" /><published>2025-01-16T00:00:00-05:00</published><updated>2025-01-16T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/16/19.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/16/19.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 19.0.0 release. This release
covers over 2 months of development work and includes <a href="https://github.com/apache/arrow/milestone/66?closed=1"><strong>202 resolved
issues</strong></a> on <a href="/release/19.0.0.html#contributors"><strong>330 distinct commits</strong></a> from <a href="/release/19.0.0.html#contributors"><strong>67 distinct
contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a> to
learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/19.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 18.1.0 release, Adam Reeve and Laurent Goujon have been invited to
become committers. Gang Wu has been invited to join the Project Management
Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>A <a href="https://github.com/apache/arrow/issues/45283">bug</a> has been identified in the
19.0.0 versions of the C++ and Python libraries which prevents reading Parquet
files written by Arrow Rust v53.0.0 or higher. The files written by Arrow Rust
are correct and the bug was in the patch adding support for Parquet’s
<a href="https://github.com/apache/parquet-format/pull/197">SizeStatistics</a> feature to
Arrow C++ and Python. See <a href="https://github.com/apache/arrow/issues/45283">#45293</a>
for more details including a potential workaround.</p>

<p>As a result, we plan to create a 19.0.1 release to include a fix for this which
should be available in next few weeks.</p>

<h2 id="columnar-format">Columnar Format</h2>

<p>We’ve added a new experimental specification for representing statistics on
Arrow Arrays as Arrow Arrays. This is useful for preserving and exchanging
statistics between systems such as when converting Parquet data to Arrow. See
<a href="https://arrow.apache.org/docs/format/StatisticsSchema.html">the statistics schema
documentation</a> for
details.</p>

<p>We’ve expanded the Arrow C Device Data Interface to include an experimental
Async Device Stream Interface. While the existing Arrow C Device Data Interface
is a pull-oriented API, the Async interface provides a push-oriented design for
other workflows. See the
<a href="https://arrow.apache.org/docs/format/CDeviceDataInterface.html#async-device-stream-interface">documentation</a>
for more information. It currently has implementations in the C++ and Go
libraries.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC Notes</h2>

<p>The precision of a Timestamp (used for timeouts) is now nanoseconds on all
platforms; previously it was platform-dependent. This may be a breaking change
depending on your use case.
(<a href="https://github.com/apache/arrow/issues/44679">#44679</a>)</p>

<p>The Python bindings now support various new fields that were added to
FlightEndpoint/FlightInfo (like <code class="language-plaintext highlighter-rouge">expiration_time</code>).
(<a href="https://github.com/apache/arrow/issues/36954">#36954</a>)</p>

<h2 id="c-notes">C++ Notes</h2>

<h3 id="compute">Compute</h3>

<ul>
  <li>It is now possible to cast from a struct type to another struct type with
additional columns, provided the additional columns are nullable
(<a href="https://github.com/apache/arrow/issues/44555">#44555)</a>.</li>
  <li>The compute function <code class="language-plaintext highlighter-rouge">expm1</code> has been added to compute <code class="language-plaintext highlighter-rouge">exp(x) - 1</code> with better
accuracy when the input value is close to 0
(<a href="https://github.com/apache/arrow/issues/44903">#44903</a>).</li>
  <li>Hyperbolic trigonometric functions and their reciprocals have also been added.
(<a href="https://github.com/apache/arrow/issues/44952">#44952</a>).</li>
  <li>The new Decimal32 and Decimal64 types have been further supported by allowing
casting between numeric, string, and other decimal types
(<a href="https://github.com/apache/arrow/issues/43956">#43956</a>).</li>
</ul>

<h3 id="acero">Acero</h3>

<ul>
  <li>Added AVX2 support for decoding row tables in the Swiss join specialization of
hash joins, enabling up to 40% performance improvement for build-heavy
workloads. (<a href="https://github.com/apache/arrow/issues/43693">#43693</a>)</li>
</ul>

<h3 id="filesystems">Filesystems</h3>

<ul>
  <li>The S3 filesystem has gained support for server-side encryption with customer
provided keys, aka SSE-C.
(<a href="https://github.com/apache/arrow/issues/43535">#43535</a>)</li>
  <li>The S3 filesystem also gained an option to disable the SIGPIPE signals that
may be emitted on some network events.
(<a href="https://github.com/apache/arrow/issues/44695">#44695</a>)</li>
  <li>The Azure filesystem now supports SAS token authentication.
(<a href="https://github.com/apache/arrow/issues/44308">#44308</a>).</li>
</ul>

<h3 id="flight-rpc">Flight RPC</h3>

<ul>
  <li>The precision of a Timestamp (used for timeouts) is now nanoseconds on all
platforms; previously it was platform-dependent. This may be a breaking change
depending on your use case.
(<a href="https://github.com/apache/arrow/issues/44679">#44679</a>)</li>
  <li>The Python bindings now support various new fields that were added to
FlightEndpoint/FlightInfo (like <code class="language-plaintext highlighter-rouge">expiration_time</code>).
(<a href="https://github.com/apache/arrow/issues/36954">#36954</a>)</li>
  <li>The UCX backend has been deprecated and is scheduled for removal.
(<a href="https://github.com/apache/arrow/issues/45079">#45079</a>)</li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>The initial footer read size can now be configured to reduce the number of
potential round-trips on hi-latency filesystems such as S3.
(<a href="https://github.com/apache/arrow/issues/45015">#45015</a>)</li>
  <li>The new <code class="language-plaintext highlighter-rouge">SizeStatistics</code> format feature has been implemented, though it is
disabled by default when writing.
(<a href="https://github.com/apache/arrow/issues/40592">#40592</a>)</li>
  <li>We’ve added a new method to the ParquetFileReader class,
<a href="https://arrow.apache.org/docs/cpp/api/formats.html#_CPPv4N7parquet17ParquetFileReader13GetReadRangesERKNSt6vectorIiEERKNSt6vectorIiEE7int64_t7int64_t">GetReadRanges</a>,
which can calculate the byte ranges necessary to read a given set of columns and
row groups. This may be useful to pre-buffer file data via caching mechanisms.
(<a href="https://github.com/apache/arrow/issues/45092">#45092</a>)</li>
  <li>We’ve added <code class="language-plaintext highlighter-rouge">arrow::Result</code>-returning variants for
<code class="language-plaintext highlighter-rouge">parquet::arrow::OpenFile()</code> and
<code class="language-plaintext highlighter-rouge">parquet::arrow::FileReader::GetRecordBatchReader()</code>.
(<a href="https://github.com/apache/arrow/issues/44784">#44784</a>,
<a href="https://github.com/apache/arrow/issues/44808">#44808</a>)</li>
</ul>

<h2 id="c-notes-1">C# Notes</h2>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">PrimitiveArrayBuilder</code> constructor has been made public to allow writing
custom builders. (<a href="https://github.com/apache/arrow/issues/23995">#23995</a>)</li>
  <li>Improved the performance of looking up schema fields by name.
(<a href="https://github.com/apache/arrow/issues/44575">#44575</a>)</li>
</ul>

<h2 id="java-go-and-rust-notes">Java, Go, and Rust Notes</h2>

<p>The Java, Go, and Rust Go projects have moved to separate repositories outside
the main Arrow <a href="https://github.com/apache/arrow">monorepo</a>.</p>

<ul>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-java">Java
implementation</a>, see the latest <a href="https://github.com/apache/arrow-java/releases">Arrow
Java changelog</a>.</li>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-rs">Rust
implementation</a> see the latest <a href="https://github.com/apache/arrow-rs/blob/main/CHANGELOG.md">Arrow Rust
changelog</a>.</li>
  <li>For notes on the latest release of the <a href="https://github.com/apache/arrow-go">Go
implementation</a>, see the latest <a href="https://github.com/apache/arrow-go/releases">Arrow Go
changelog</a>.</li>
</ul>

<h2 id="linux-packaging-notes">Linux Packaging Notes</h2>

<ul>
  <li>Debian: Fixed keyring format to support newer libapt (e.g., used by
Trixie). (<a href="https://github.com/apache/arrow/issues/45118">#45118</a>)</li>
</ul>

<h2 id="python-notes">Python Notes</h2>

<p>New features:</p>

<ul>
  <li>The upcoming pandas 3.0 <a href="https://pandas.pydata.org/pdeps/0014-string-dtype.html">string
dtype</a> is now
supported by PyArrow’s <code class="language-plaintext highlighter-rouge">to_pandas</code> routine. In the future, when using pandas &gt;=3.0,
the new pandas behavior will be enabled by default. You can opt into
the new behavior under pandas &gt;=2.3 by setting <code class="language-plaintext highlighter-rouge">pd.options.future.infer_string
= True</code>. This may be considered a breaking change.
(<a href="https://github.com/apache/arrow/issues/43683">#43683</a>)</li>
  <li>Support for 32-bit and 64-bit decimal types was added.
(<a href="https://github.com/apache/arrow/issues/44713">#44713</a>)</li>
  <li>Arrow PyCapsule stream objects are supported in <code class="language-plaintext highlighter-rouge">write_dataset</code>.
(<a href="https://github.com/apache/arrow/issues/43410">#43410</a>)</li>
  <li>New Flight features have been exposed.
(<a href="https://github.com/apache/arrow/issues/36954">#36954</a>)</li>
  <li>Bindings for <code class="language-plaintext highlighter-rouge">JsonExtensionType</code> and <code class="language-plaintext highlighter-rouge">JsonArray</code> were added.
(<a href="https://github.com/apache/arrow/issues/44066">#44066</a>)</li>
  <li>Hyperbolic trigonometry functions added to the Arrow C++ compute kernels are
also available in PyArrow.
(<a href="https://github.com/apache/arrow/issues/44952">#44952</a>)</li>
</ul>

<p>Other improvements:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">strings_to_categorical</code> keyword in <code class="language-plaintext highlighter-rouge">to_pandas</code> can now be used for string
view type. (<a href="https://github.com/apache/arrow/issues/45175">#45175</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">from_buffers</code> is updated to work with <code class="language-plaintext highlighter-rouge">StringView</code>.
(<a href="https://github.com/apache/arrow/issues/44651">#44651</a>)</li>
  <li>Version suffixes are also set for Arrow Python C++ (<code class="language-plaintext highlighter-rouge">libarrow_python*</code>)
libraries. (<a href="https://github.com/apache/arrow/issues/44614">#44614</a>)</li>
</ul>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib Notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Added basic support for JRuby with an implementation based on Arrow Java
(<a href="https://github.com/apache/arrow/pull/44346">#44346</a>). The plan is to release
this as a gem once it covers a base set of features. See
<a href="https://github.com/apache/arrow/issues/45324">#45324</a> for more information.</li>
  <li>Added support for 32bit and 64bit decimal, binary view, and string view. See
<a href="https://github.com/apache/arrow/issues?q=is%3Aclosed%20milestone%3A19.0.0%20label%3A%22Component%3A%20GLib%22">issues
listed</a>
in the 19.0.0 milestone for more details.</li>
  <li>Fixed a bug that empty struct list can’t be built.
(<a href="https://github.com/apache/arrow/issues/44742">#44742</a>)</li>
  <li>Fixed a bug that <code class="language-plaintext highlighter-rouge">record_batch[:column].size</code> raises an exception.
(<a href="https://github.com/apache/arrow/issues/45119">#45119</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Added support for 32bit and 64bit decimal, binary view, and string view. See
<a href="https://github.com/apache/arrow/issues?q=is%3Aclosed%20milestone%3A19.0.0%20label%3A%22Component%3A%20GLib%22">issues listed in the 19.0.0
milestone</a>
for more details.</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 19.0.0 release. This release covers over 2 months of development work and includes 202 resolved issues on 330 distinct commits from 67 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 18.1.0 release, Adam Reeve and Laurent Goujon have been invited to become committers. Gang Wu has been invited to join the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Release Highlights A bug has been identified in the 19.0.0 versions of the C++ and Python libraries which prevents reading Parquet files written by Arrow Rust v53.0.0 or higher. The files written by Arrow Rust are correct and the bug was in the patch adding support for Parquet’s SizeStatistics feature to Arrow C++ and Python. See #45293 for more details including a potential workaround. As a result, we plan to create a 19.0.1 release to include a fix for this which should be available in next few weeks. Columnar Format We’ve added a new experimental specification for representing statistics on Arrow Arrays as Arrow Arrays. This is useful for preserving and exchanging statistics between systems such as when converting Parquet data to Arrow. See the statistics schema documentation for details. We’ve expanded the Arrow C Device Data Interface to include an experimental Async Device Stream Interface. While the existing Arrow C Device Data Interface is a pull-oriented API, the Async interface provides a push-oriented design for other workflows. See the documentation for more information. It currently has implementations in the C++ and Go libraries. Arrow Flight RPC Notes The precision of a Timestamp (used for timeouts) is now nanoseconds on all platforms; previously it was platform-dependent. This may be a breaking change depending on your use case. (#44679) The Python bindings now support various new fields that were added to FlightEndpoint/FlightInfo (like expiration_time). (#36954) C++ Notes Compute It is now possible to cast from a struct type to another struct type with additional columns, provided the additional columns are nullable (#44555). The compute function expm1 has been added to compute exp(x) - 1 with better accuracy when the input value is close to 0 (#44903). Hyperbolic trigonometric functions and their reciprocals have also been added. (#44952). The new Decimal32 and Decimal64 types have been further supported by allowing casting between numeric, string, and other decimal types (#43956). Acero Added AVX2 support for decoding row tables in the Swiss join specialization of hash joins, enabling up to 40% performance improvement for build-heavy workloads. (#43693) Filesystems The S3 filesystem has gained support for server-side encryption with customer provided keys, aka SSE-C. (#43535) The S3 filesystem also gained an option to disable the SIGPIPE signals that may be emitted on some network events. (#44695) The Azure filesystem now supports SAS token authentication. (#44308). Flight RPC The precision of a Timestamp (used for timeouts) is now nanoseconds on all platforms; previously it was platform-dependent. This may be a breaking change depending on your use case. (#44679) The Python bindings now support various new fields that were added to FlightEndpoint/FlightInfo (like expiration_time). (#36954) The UCX backend has been deprecated and is scheduled for removal. (#45079) Parquet The initial footer read size can now be configured to reduce the number of potential round-trips on hi-latency filesystems such as S3. (#45015) The new SizeStatistics format feature has been implemented, though it is disabled by default when writing. (#40592) We’ve added a new method to the ParquetFileReader class, GetReadRanges, which can calculate the byte ranges necessary to read a given set of columns and row groups. This may be useful to pre-buffer file data via caching mechanisms. (#45092) We’ve added arrow::Result-returning variants for parquet::arrow::OpenFile() and parquet::arrow::FileReader::GetRecordBatchReader(). (#44784, #44808) C# Notes The PrimitiveArrayBuilder constructor has been made public to allow writing custom builders. (#23995) Improved the performance of looking up schema fields by name. (#44575) Java, Go, and Rust Notes The Java, Go, and Rust Go projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Java implementation, see the latest Arrow Java changelog. For notes on the latest release of the Rust implementation see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog. Linux Packaging Notes Debian: Fixed keyring format to support newer libapt (e.g., used by Trixie). (#45118) Python Notes New features: The upcoming pandas 3.0 string dtype is now supported by PyArrow’s to_pandas routine. In the future, when using pandas &gt;=3.0, the new pandas behavior will be enabled by default. You can opt into the new behavior under pandas &gt;=2.3 by setting pd.options.future.infer_string = True. This may be considered a breaking change. (#43683) Support for 32-bit and 64-bit decimal types was added. (#44713) Arrow PyCapsule stream objects are supported in write_dataset. (#43410) New Flight features have been exposed. (#36954) Bindings for JsonExtensionType and JsonArray were added. (#44066) Hyperbolic trigonometry functions added to the Arrow C++ compute kernels are also available in PyArrow. (#44952) Other improvements: strings_to_categorical keyword in to_pandas can now be used for string view type. (#45175) from_buffers is updated to work with StringView. (#44651) Version suffixes are also set for Arrow Python C++ (libarrow_python*) libraries. (#44614) Ruby and C GLib Notes Ruby Added basic support for JRuby with an implementation based on Arrow Java (#44346). The plan is to release this as a gem once it covers a base set of features. See #45324 for more information. Added support for 32bit and 64bit decimal, binary view, and string view. See issues listed in the 19.0.0 milestone for more details. Fixed a bug that empty struct list can’t be built. (#44742) Fixed a bug that record_batch[:column].size raises an exception. (#45119) C GLib Added support for 32bit and 64bit decimal, binary view, and string view. See issues listed in the 19.0.0 milestone for more details.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.1.0 Release</title><link href="https://arrow.apache.org/blog/2025/01/13/arrow-go-18.1.0/" rel="alternate" type="text/html" title="Apache Arrow Go 18.1.0 Release" /><published>2025-01-13T00:00:00-05:00</published><updated>2025-01-13T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/13/arrow-go-18.1.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/13/arrow-go-18.1.0/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the <a href="https://github.com/apache/arrow-go/releases/tag/v18.1.0">v18.1.0</a> release of Apache Arrow Go. 
This minor release covers 32 commits from 6 distinct contributors.</p>

<h2 id="contributors">Contributors</h2>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> v18.0.0..v18.1.0
<span class="go">    24	Matt Topol
     2	Sutou Kouhei
     2	Todd Treece
     1	Nick Ripley
     1	Raúl Cumplido
     1	Ruihao Chen
</span></code></pre></div></div>

<h2 id="changelog">Changelog</h2>

<h3 id="whats-changed">What’s Changed</h3>
<ul>
  <li>GH-19: Add Code Of Conduct by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/161">#161</a></li>
  <li>GH-22: Add issue template by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/163">#163</a></li>
  <li>GH-26: Add Codeowners by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/160">#160</a></li>
  <li>GH-41: [Array] Make String and Binary consistent by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/165">#165</a></li>
  <li>fix(arrow/compute): fix scenario where prealloc output is missed by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/167">#167</a></li>
  <li>fix: don’t clobber saved frame pointer in arm64 assembly functions by @nsrip-dd in <a href="https://github.com/apache/arrow-go/pull/170">#170</a></li>
  <li>fix(parquet): Reading UUID columns by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/173">#173</a></li>
  <li>fix(arrow/compute): compare kernels with UUID by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/174">#174</a></li>
  <li>GH-23: Add PR template by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/164">#164</a></li>
  <li>ci: add self-assignable issues by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/176">#176</a></li>
  <li>feat(arrow/compute): Implement kernel for “not” function by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/178">#178</a></li>
  <li>fix(arrow/cdata): handle export struct with no fields by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/175">#175</a></li>
  <li>feat(arrow/compute): make is_nan dispatchable by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/177">#177</a></li>
  <li>GH-20: Add CONTRIBUTING.md by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/162">#162</a></li>
  <li>chore: bump google.golang.org/grpc from 1.67.1 to 1.68.0 by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/190">#190</a></li>
  <li>feat(arrow/cdata): Add Implementation of Async C Data interface by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/169">#169</a></li>
  <li>fix(arrow/memory/internal/cgoalloc): Remove usage of reflect.SliceHeader by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/194">#194</a></li>
  <li>ci: update pkg-config to pkgconf on Homebrew by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/198">#198</a></li>
  <li>ci: use correct GitHub Actions permission “contents” instead of “content” by @raulcd in <a href="https://github.com/apache/arrow-go/pull/200">#200</a></li>
  <li>feat(arrow/ipc): add functions to generate payloads by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/196">#196</a></li>
  <li>fix(internal/json): add arrow_json_stdlib build tag by @toddtreece in <a href="https://github.com/apache/arrow-go/pull/199">#199</a></li>
  <li>feat(parquet/pqarrow): Add ForceLarge option by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/197">#197</a></li>
  <li>fix(parquet/metadata): fix default unsigned int statistics by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/210">#210</a></li>
  <li>chore: bump github.com/goccy/go-json to v0.10.4 by @toddtreece in <a href="https://github.com/apache/arrow-go/pull/218">#218</a></li>
  <li>feat(parquet): Move footerOffset into FileMetaData by @joechenrh in <a href="https://github.com/apache/arrow-go/pull/217">#217</a></li>
  <li>feat(arrow/ipc): implement lazy loading/zero-copy for IPC files by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/216">#216</a></li>
  <li>chore(arrow/compute/expr): upgrade substrait-go by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/234">#234</a></li>
  <li>chore: Update the project URL in apache/arrow-go by @kou in <a href="https://github.com/apache/arrow-go/pull/239">#239</a></li>
  <li>chore: Enable Java integration test by @kou in <a href="https://github.com/apache/arrow-go/pull/240">#240</a></li>
  <li>feat(parquet): Add support for Page Indexes by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/223">#223</a></li>
  <li>chore: bump version by @zeroshade in <a href="https://github.com/apache/arrow-go/pull/241">#241</a></li>
</ul>

<h3 id="new-contributors">New Contributors</h3>
<ul>
  <li>@nsrip-dd made their first contribution in <a href="https://github.com/apache/arrow-go/pull/170">#170</a></li>
  <li>@toddtreece made their first contribution in <a href="https://github.com/apache/arrow-go/pull/199">#199</a></li>
  <li>@joechenrh made their first contribution in <a href="https://github.com/apache/arrow-go/pull/217">#217</a></li>
</ul>

<p><strong>Full Changelog</strong>: <a href="https://github.com/apache/arrow-go/compare/v18.0.0...v18.1.0">changelog</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.1.0 release of Apache Arrow Go. This minor release covers 32 commits from 6 distinct contributors. Contributors $ git shortlog -sn v18.0.0..v18.1.0 24 Matt Topol 2 Sutou Kouhei 2 Todd Treece 1 Nick Ripley 1 Raúl Cumplido 1 Ruihao Chen Changelog What’s Changed GH-19: Add Code Of Conduct by @zeroshade in #161 GH-22: Add issue template by @zeroshade in #163 GH-26: Add Codeowners by @zeroshade in #160 GH-41: [Array] Make String and Binary consistent by @zeroshade in #165 fix(arrow/compute): fix scenario where prealloc output is missed by @zeroshade in #167 fix: don’t clobber saved frame pointer in arm64 assembly functions by @nsrip-dd in #170 fix(parquet): Reading UUID columns by @zeroshade in #173 fix(arrow/compute): compare kernels with UUID by @zeroshade in #174 GH-23: Add PR template by @zeroshade in #164 ci: add self-assignable issues by @zeroshade in #176 feat(arrow/compute): Implement kernel for “not” function by @zeroshade in #178 fix(arrow/cdata): handle export struct with no fields by @zeroshade in #175 feat(arrow/compute): make is_nan dispatchable by @zeroshade in #177 GH-20: Add CONTRIBUTING.md by @zeroshade in #162 chore: bump google.golang.org/grpc from 1.67.1 to 1.68.0 by @zeroshade in #190 feat(arrow/cdata): Add Implementation of Async C Data interface by @zeroshade in #169 fix(arrow/memory/internal/cgoalloc): Remove usage of reflect.SliceHeader by @zeroshade in #194 ci: update pkg-config to pkgconf on Homebrew by @zeroshade in #198 ci: use correct GitHub Actions permission “contents” instead of “content” by @raulcd in #200 feat(arrow/ipc): add functions to generate payloads by @zeroshade in #196 fix(internal/json): add arrow_json_stdlib build tag by @toddtreece in #199 feat(parquet/pqarrow): Add ForceLarge option by @zeroshade in #197 fix(parquet/metadata): fix default unsigned int statistics by @zeroshade in #210 chore: bump github.com/goccy/go-json to v0.10.4 by @toddtreece in #218 feat(parquet): Move footerOffset into FileMetaData by @joechenrh in #217 feat(arrow/ipc): implement lazy loading/zero-copy for IPC files by @zeroshade in #216 chore(arrow/compute/expr): upgrade substrait-go by @zeroshade in #234 chore: Update the project URL in apache/arrow-go by @kou in #239 chore: Enable Java integration test by @kou in #240 feat(parquet): Add support for Page Indexes by @zeroshade in #223 chore: bump version by @zeroshade in #241 New Contributors @nsrip-dd made their first contribution in #170 @toddtreece made their first contribution in #199 @joechenrh made their first contribution in #217 Full Changelog: changelog]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrowフォーマットはどのようにクエリー結果の転送を高速にしているのか</title><link href="https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer-japanese/" rel="alternate" type="text/html" title="Apache Arrowフォーマットはどのようにクエリー結果の転送を高速にしているのか" /><published>2025-01-10T00:00:00-05:00</published><updated>2025-01-10T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer-japanese</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer-japanese/"><![CDATA[<!--

-->

<p><em>この記事はデータベースとクエリーエンジン間のデータ交換フォーマットとしてなぜArrowが使われているのかという謎を解くシリーズの最初の記事です。</em></p>

<p><img src="/img/arrow-result-transfer/part-1-banner.png" width="100%" class="img-responsive" alt="" aria-hidden="true" /></p>

<p>「どうしてこんなに時間がかかるの？」</p>

<p>これはデータを扱っている人がクエリー結果を待っている間によく考える質問です。たくさんの回答が考えられます。もしかしたら、データソースが適切にパーティショニングされていないかもしれません。もしかしたら、SaaSのデータウェアハウスのリソースが足りないのかもしれません。もしかしたら、クエリーオプティマイザーがSQL文を効率的な実行計画に落とし込めなかったのかもしれません。</p>

<p>しかし、驚くほど多くの場合、クエリ結果をクライアントに転送するために非効率なプロトコルを使っていることが答えになります。<a href="https://www.vldb.org/pvldb/vol10/p1022-muehleisen.pdf" target="_blank">2017年の論文</a>では、Mark RaasveldtとHannes Mühleisenは、特にクエリー結果が大きい場合、クエリー結果の転送がクエリー実行時間の大部分を占めることがよくあることを観測しました。しかし、ボトルネックはあなたが予想している場所ではありません。</p>

<p>送信元から受信先へのクエリー結果の転送は次の3つのステップになります。</p>

<ol>
  <li>送信元：結果を元のフォーマットから転送用のフォーマットにシリアライズ</li>
  <li>送信元：転送用のフォーマットでネットワーク経由でデータを送信<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></li>
  <li>受信先：転送用のフォーマットのデータを扱いやすいフォーマットにデシリアライズ</li>
</ol>

<p>ネットワークが遅かった時代は、通常はデータ転送ステップ（ステップ2）がボトルネックでした。そのため、シリアライズステップ（ステップ1）とデシリアライズステップ（ステップ3）を高速にする動機はほとんどありませんでした。その代わり、転送するデータを小さくすることに重点がおかれていました。多くの場合は転送時間を短縮するためにデータを圧縮していました。広く使われているデータベース接続API（ODBCとJDBC）とデータベースクライアントプロトコル（MySQLのクライアント・サーバープロトコルやPostgreSQLのフロントエンド・バックエンドプロトコル）はこの時代に設計されました。しかし、ネットワークは高速になり、転送時間が短縮されたため、ボトルネックはシリアライズステップとデシリアライズステップに移ってきました<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>。これは大きな結果を生成するクエリーで特に当てはまります。多くのデータエンジニアリングパイプライン・データ分析パイプラインではこのようなクエリーがよく実行されます。</p>

<p>今でも多くのクエリー結果はレガシーなAPI・プロトコルで転送されています。これらのAPI・プロトコルでは非効率な転送用のフォーマットを使わないといけないので、多くのシリアライズ・デシリアライズオーバーヘッドがあります。<a href="https://www.vldb.org/pvldb/vol14/p534-li.pdf" target="_blank">2021年の論文</a>では、Tianyu LiらはODBCとPostgreSQLプロトコルを使う例を示しました。この例では、総クエリー実行時間の99.996%がシリアライズ・デシリアライズに費やされていました。これは極端なケースですが、90%以上費やされている実例をたくさん見てきました。今では、データエンジニアリングクエリー・データ分析クエリーのために、シリアライズ・デシリアライズを高速にする転送用フォーマットを選択する強い動機があります。</p>

<p>そこでArrowですよ！</p>

<p>Apache Arrowオープンソースプロジェクトは<a href="https://arrow.apache.org/docs/format/Columnar.html" target="_blank">データフォーマット</a>を定義しています。このフォーマットはクエリー結果転送時のシリアライズ・デシリアライズを高速にするために設計されています。多くの場合は、シリアライズ・デシリアライズは不要になります。2016年に作られて以来、Arrowフォーマットおよびこのフォーマットを中心に構築された多言語対応ツールボックスは広く使われるようになりました。しかし、Arrowがどのようにシリアライズ・デシリアライズのオーバーヘッドを削減しているかの技術的な詳細はあまり理解されていません。この問題を解決するために、シリアライズ・デシリアライズのオーバーヘッドを削減することを可能にするArrowフォーマットの5つの特徴を説明します。</p>

<h3 id="1-arrowフォーマットは列指向">1. Arrowフォーマットは列指向</h3>

<p>列指向（カラムナー）データフォーマットは各カラムの値をメモリー上の連続した領域に保持します。これは行指向データフォーマットとは対象的です。行指向データフォーマットは各行の値をメモリー上の連続した領域に保持します。</p>

<figure style="text-align: center;">
  <img src="/img/arrow-result-transfer/part-1-figure-1-row-vs-column-layout.png" width="100%" class="img-responsive" alt="図1：3行5列のテーブルの物理メモリーレイアウトは行指向と列指向でどのように違うのか。" />
  <figcaption>図1：3行5列のテーブルの物理メモリーレイアウトは行指向と列指向でどのように違うのか。</figcaption>
</figure>

<p>高性能な分析データベース・データウェアハウス・クエリーエンジン・ストレージシステムは列指向アーキテクチャーを採用することが多いです。これは、よく使われる分析クエリーを高速に実行するためです。最新の列指向クエリーシステムは、Amazon Redshift・Apache DataFusion・ClickHouse・Databricks Photon Engine・DuckDB・Google BigQuery・Microsoft Azure Synapse Analytics・OpenText Analytics Database (Vertica)・Snowflake・Voltron Data Theseusなどです。</p>

<p>同様に、分析用クエリー結果の多くの出力先も列指向アーキテクチャーを採用しています。出力先は、たとえば、BIツール・データアプリケーションプラットフォーム・データフレームライブラリー・機械学習プラットフォームなどです。列指向のBIツールは、Amazon QuickSight・Domo・GoodData・Power BI・Qlik Sense・Spotfire・Tableauなどです。列指向のデータフレームライブラリーは、cuDF・pandas・Polarsなどです。</p>

<p>そのため、クエリー結果の送信元のフォーマットと受信先のフォーマットがどちらも列指向フォーマットであることがますます一般的になっています。列指向の送信元と列指向の受信先でもっとも効率的にデータを転送する方法は列指向の転送用フォーマットを使うことです。これにより、行と列を転置するという時間のかかる処理をせずに済みます。行指向の転送用フォーマットを使うと、データ転送元のシリアライズ処理で列を行に転置し、データ受信先のデシリアライズ処理で行を列に転置しないといけません。</p>

<p>Arrowは列指向のデータフォーマットです。Arrowフォーマットのデータの列指向のレイアウトは、広く使われている送信元システム・受信先システムでのデータのレイアウトと似ています。多くのケースでは似ているのではなく同一のレイアウトになっています。</p>

<h3 id="2-arrowフォーマットは自己記述的で型安全">2. Arrowフォーマットは自己記述的で型安全</h3>

<p>自己記述的なデータフォーマットではスキーマ（カラムの名前と型）とスキーマ以外のメタデータ（データの構造を説明するメタデータ）は、データの中に入っています。自己記述的なフォーマットはデータを安全かつ効率的に処理するために必要なすべての情報を受信したシステムに提供します。一方、自己記述的ではないフォーマットでは、受信したシステムはスキーマや構造を推測する（これは遅くエラーが発生しやすい処理）か、別途スキーマを取得しなければいけません。</p>

<p>いくつかの自己記述的なデータフォーマットの重要な特徴は型安全を強制することです。フォーマットが型安全を強制する場合、データの値は指定された型に準拠していることが保証されるため、受信したシステムはデータを処理するときに型エラーが発生する可能性を排除できます。一方、型安全を強制しないフォーマットでは、受信したシステムはデータ内の各値の有効性をチェックする（これは計算コストが高い処理）か、データを処理するときに型エラーを処理する必要があります。</p>

<p>自己記述的でなく型安全でもないCSVのようなデータを読むとき、データのスキャン・推測・チェックのすべての処理がデシリアライズの大きなオーバーヘッドになります。さらに悪いことに、このようなフォーマットはあいまいさ・デバッグの問題・メンテナンスの課題・セキュリティの脆弱性につながる可能性があります。</p>

<p>Arrowフォーマットは自己記述的で型安全を強制します。さらに、Arrowの型システムは広く使われているデータ送信元・受信先と似ています。多くのケースでは同じかそれらのスーパーセット（それらよりも表現力が高い）になっています。ここでのデータ送信元・データ受信先には、ほぼすべての列指向データシステムと多くの行指向データシステムが含まれています。たとえば、Apache Sparkやリレーショナル・データベースなどが含まれています。Arrowフォーマットを使うと、これらのシステムはネイティブな型と対応するArrowの型の間ですばやく安全にデータの値を変換できます。</p>

<h3 id="3-arrowフォーマットはゼロコピー可能">3. Arrowフォーマットはゼロコピー可能</h3>

<p>ゼロコピー操作とは、中間コピーをまったく作成せずにあるメディアから別のメディアにデータを転送する操作です。データフォーマットがゼロコピー操作をサポートしている場合、メモリー上の構造はディスク上あるいはネットワーク上の構造と同じになります。そのため、たとえば、メモリー上で使える構造のデータをネットワークから直接読み込めます。このとき、中間コピーや変換をする必要はありません。</p>

<p>Arrowフォーマットはゼロコピー操作をサポートしています。データの値の集まりを保持するために、Arrowは<a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc" target="_blank">レコードバッチ</a>と呼んでいる列指向の表形式のデータ構造を定義しています。Arrowのレコードバッチはメモリー上に保持することもできますし、ネットワーク経由で送信することもできますし、ディスクに保存することもできます。レコードバッチがどのメディアにあってもどのシステムで生成されてもバイナリー構造は変わりません。スキーマと他のメタデータを保存するために、ArrowはFlatBuffersを使っています。FlatBuffersはGoogleが作ったフォーマットです。FlatBuffersも、どのメディア上でも同じバイナリー構造になります。</p>

<p>これらの設計判断により、Arrowは転送用のフォーマットとしてだけでなく、メモリー上のフォーマットとしてもディスク上のフォーマットとしても使えます。これは、JSONやCSVといったテキストベースのフォーマットやProtocol BuffersやThriftといったシリアライズされたバイナリーフォーマットとは対照的です。これらのフォーマットは専用の構文を使ってデータをエンコードします。これらのフォーマットのデータをメモリー上で使える構造にロードするには、データをパースしてデコードする必要があります。これはParquetやORCといったバイナリーフォーマットとも対照的です。これらはディスク上でのデータサイズを削減するためにエンコードしたり圧縮したりします。これらのフォーマットのデータをメモリー上で使える構造にロードするためには、展開してデコードする必要があります<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>。</p>

<p>データ送信元のシステムでは、メモリー上あるいはディスク上にArrowフォーマットのデータがあればシリアライズせずにArrowフォーマットでネットワーク越しにデータ転送できるということです。また、データ受信先のシステムでは、デシリアライズせずにネットワークからメモリー上にデータを読み込んだりディスク上にArrowファイルとして書き出したりできるということです。</p>

<p>Arrowフォーマットは非常に効率よく分析操作できるメモリー上のフォーマットとして設計されています。このため、多くの列指向データシステムは内部のメモリー上のフォーマットとしてArrowを採用しています。たとえば、Apache DataFusion・cuDF・Dremio・InfluxDB・Polars・Velox・Voltron Data Theseusなどが採用しています。これらのシステムがデータ送信元あるいはデータ受信先である場合、シリアライズ・デシリアライズのオーバーヘッドは完全になくなります。他の多くの列指向のデータシステムの場合、それらが使っているプロプライエタリなメモリー上のフォーマットはArrowと非常に似ています。それらのシステムでは、Arrowフォーマットとのシリアライズ・デシリアライズ処理は高速で効率的です。</p>

<h3 id="4-arrowフォーマットはストリーム可能">4. Arrowフォーマットはストリーム可能</h3>

<p>ストリーム可能なデータフォーマットはデータセット全体を待たずに1つのチャンクずつ順番に処理できます。データがストリーム可能なフォーマットで転送されているとき、受信先のシステムは最初のチャンクが到着したらすぐに処理を開始できます。これによりいくつかの方法でデータ転送を高速化できます。たとえば、データを処理している間に次のデータを受信できます。たとえば、受信先のシステムはメモリーをより効率的に使うことができます。たとえば、複数のストリームを並列に転送することができます。これにより、データ転送・データのデシリアライズ・データ処理を高速化できます。</p>

<p>たとえば、CSVはストリーム可能なデータフォーマットです。なぜなら、（もし含まれているなら）ファイルの先頭のヘッダーにカラム名があって、ファイル中のそれ以降の行は順番に処理できるからです。一方、ParquetとORCはストリーム可能ではないデータフォーマットです。なぜなら、データを処理するために必要なスキーマと他のメタデータがファイルの最後のフッターにあるからです。処理を始める前にファイル全体をダウンロードする（あるいはファイルの最後まで移動してフッターを別途ダウンロードする）必要があります<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>。</p>

<p>Arrowはストリーム可能なデータフォーマットです。データセットは同じスキーマを持つレコードバッチの列としてArrowで表現できます。Arrowは<a href="https://arrow.apache.org/docs/format/Columnar.html#ipc-streaming-format" target="_blank">ストリーミングフォーマット</a>を定義しています。このフォーマットでは、まずスキーマがあり、そのあと、1つ以上のレコードバッチがあります。Arrowストリームを受信するシステムは受信した順にレコードバッチを処理できます。</p>

<figure style="text-align: center;">
  <img src="/img/arrow-result-transfer/part-1-figure-2-arrow-stream.png" width="100%" class="img-responsive" alt="図2：3列の表を転送するArrowストリーム。最初のレコードバッチは最初の3行の値だけを含み、2つめのレコードバッチは次の3行のデータが含まれている。実際のArrowレコードバッチには数千から数百万行が含まれていることもある。" />
  <figcaption>図2：3列の表を転送するArrowストリーム。最初のレコードバッチは最初の3行の値だけを含み、2つめのレコードバッチは次の3行のデータが含まれている。実際のArrowレコードバッチには数千から数百万行が含まれていることもある。</figcaption>
</figure>

<h3 id="5-arrowフォーマットは汎用的">5. Arrowフォーマットは汎用的</h3>

<p>Arrowは表形式のデータをメモリー上で扱うためのデファクトスタンダードなフォーマットとして登場しました。Arrowは言語に依存しないオープンな標準です。様々な言語向けにArrowデータを扱うためのライブラリーが用意されています。たとえば、C・C++・C#・Go・Java・JavaScript・Julia・MATLAB・Python・R・Ruby・Rust・Swift用の公式ライブラリーがあります。主流の言語で開発されているアプリケーションはArrowフォーマットでデータを送受信する機能をサポートできます。JDBCなどのいくつかデータベース接続APIでは特定の言語のランタイムを使わないといけませんが、Arrowフォーマットではそのような必要はありません。</p>

<p>Arrowの汎用性により、実際のデータシステムを高速化する際の基本的な問題に対処できます。その問題とは、性能向上はシステムのボトルネックに律速するということです。この問題は<a href="https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%A0%E3%83%80%E3%83%BC%E3%83%AB%E3%81%AE%E6%B3%95%E5%89%87" target="_blank">Amdahlの法則</a>として知られています。実際のデータパイプラインでは、クエリー結果が複数のステージを流れることはよくあり、各ステージでシリアライズ・デシリアライズのオーバーヘッドがあります。たとえば、もし、あなたのデータパイプラインに5つのステージがあり、そのうちの4つのステージでシリアライズ・デシリアライズオーバーヘッドを取り除くことができたとしても、あなたのシステムは速くならないでしょう。なぜなら、残った1ステージのシリアライズ・デシリアライズがパイプライン全体のボトルネックになるからです。</p>

<p>Arrowはどんな技術スタック上でも効率的に動くので、この問題の解決に役立ちます。たとえば、こんなデータフローがあったとしたらどうでしょう。NVIDIAのGPUを積んだワーカーを持つScalaベースの分散バックエンド→JettyベースのHTTPサーバー→Pyodideベースのブラウザーフロントエンドを持つNode.jsベースの機械学習フレームワークを使ってユーザーとやりとりするRailsベースの特徴量エンジニアリングアプリ。問題ありません。Arrowライブラリーはこれらのすべてのコンポーネント間のシリアライズ・デシリアライズオーバーヘッドを取り除けます。</p>

<h3 id="まとめ">まとめ</h3>

<p>より多くの商用・オープンソースのツールがArrowに対応するにつれ、シリアライズ・デシリアライズのないあるいは少ない高速なクエリー転送がますます一般的になっています。現在では、多くのデータベース・データプラットフォーム・クエリーエンジンがArrowフォーマットでクエリー結果を転送できます。たとえば、Databricks・Dremio・Google BigQuery・InfluxDB・Snowflake・Voltron Data Theseusといった商用プロダクトや、Apache DataFusion・Apache Doris・Apache Spark・ClickHouse・DuckDBといったオープンソースプロダクトがサポートしています。これにより大幅に高速化しています。</p>

<ul>
  <li>Apache Doris: <a href="https://doris.apache.org/blog/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer" target="_blank">「20倍から数百倍」高速化</a></li>
  <li>Google BigQuery: <a href="https://medium.com/google-cloud/announcing-google-cloud-bigquery-version-1-17-0-1fc428512171" target="_blank">最大「31倍高速化」</a></li>
  <li>Dremio: <a href="https://www.dremio.com/press-releases/dremio-announces-support-for-apache-arrow-flight-high-performance-data-transfer/" target="_blank">「10倍以上高速化」</a></li>
  <li>DuckDB: <a href="https://duckdb.org/2023/08/04/adbc.html#benchmark-adbc-vs-odbc" target="_blank">「38倍」高速化</a></li>
  <li>Snowflake: <a href="https://www.snowflake.com/en/blog/fetching-query-results-from-snowflake-just-got-a-lot-faster-with-apache-arrow/" target="_blank">「最大10倍」高速化</a></li>
</ul>

<p>データ受信側では、データを扱っている人はArrowベースのツール・ライブラリー・インターフェイス・プロトコルを使うことでこの高速化を最大化できます。2025年には、より多くのプロジェクトとベンダーが<a href="https://arrow.apache.org/adbc/" target="_blank">ADBC</a>標準をサポートして、Arrowフォーマットでクエリー結果を受け取ることができるツールの数が増大するでしょう。</p>

<p>このシリーズの今後の記事を楽しみにしていてください。Arrowフォーマットと他のデータフォーマットを比較して、Arrowフォーマットで結果を取得するためにクライアントが使うことのできるプロトコルとAPIを説明します。</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>転送用のフォーマットは「ワイヤーフォーマット」や「シリアライズフォーマット」と呼ばれることもあります。 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>1990年代から現在に至るまで、ネットワーク性能の向上はCPU性能の向上を上回っています。たとえば、1990年代後半は、主流のデスクトップCPUはおおよそ1GFLOPSで処理でき、一般的なWANの接続速度は56Kb/sでした。現在、主流のデスクトップCPUはおおよそ100GFLOPSで処理でき、一般的なWANの接続速度は1Gb/sです。つまり、CPUの性能はおよそ100倍向上していて、ネットワーク速度はおよそ1万倍向上しています。 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>これはアーカイブ用のストレージのようなアプリケーションにおいてもParquetやORCよりArrowの方が速いということではありません。このシリーズの今後の記事では、ArrowフォーマットとParquet・ORC・それ以外のフォーマットをより技術的な詳細に踏み込んで比較します。そして、これらが相互に補完しあうものであることを説明します。 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>これはCSVがParquet・ORCよりも高速に結果を転送できるということではありません。CSVとParquet・ORCの転送性能を比べるときは、通常、この特徴よりもここで説明している他の4つの特徴の影響の方が大きいです。 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ian Cook, David Li, Matt Topol</name></author><category term="application" /><summary type="html"><![CDATA[Arrowはシリアライズ・デシリアライズのオーバーヘッドを削減することでクエリー結果の転送を高速にします。これに関するArrowフォーマットの5つの特徴を説明します。]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-result-transfer/part-1-share-image.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-result-transfer/part-1-share-image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How the Apache Arrow Format Accelerates Query Result Transfer</title><link href="https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer/" rel="alternate" type="text/html" title="How the Apache Arrow Format Accelerates Query Result Transfer" /><published>2025-01-10T00:00:00-05:00</published><updated>2025-01-10T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer</id><content type="html" xml:base="https://arrow.apache.org/blog/2025/01/10/arrow-result-transfer/"><![CDATA[<!--

-->

<p><em>This is the first in a series of posts that aims to demystify the use of Arrow as a data interchange format for databases and query engines.</em></p>

<p><img src="/img/arrow-result-transfer/part-1-banner.png" width="100%" class="img-responsive" alt="" aria-hidden="true" /></p>

<p>“Why is this taking so long?”</p>

<p>This is a question that data practitioners often ponder while waiting for query results. It’s a question with many possible answers. Maybe your data source is poorly partitioned. Maybe your SaaS data warehouse is undersized. Maybe the query optimizer failed to translate your SQL statement into an efficient execution plan.</p>

<p>But surprisingly often, the answer is that you are using an inefficient protocol to transfer query results to the client. In a <a href="https://www.vldb.org/pvldb/vol10/p1022-muehleisen.pdf" target="_blank">2017 paper</a>, Mark Raasveldt and Hannes Mühleisen observed that query result transfer time often dominates query execution time, especially for larger results. However, the bottleneck is not where you might expect.</p>

<p>Transferring a query result from a source to a destination involves three steps:</p>

<ol>
  <li>At the source, serialize the result from its original format into a transfer format.</li>
  <li>Transmit the data over the network in the transfer format.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></li>
  <li>At the destination, deserialize the transfer format into the target format.</li>
</ol>

<p>In the era of slower networks, the transmission step was usually the bottleneck, so there was little incentive to speed up the serialization and deserialization steps. Instead, the emphasis was on making the transferred data smaller, typically using compression, to reduce the transmission time. It was during this era that the most widely used database connectivity APIs (ODBC and JDBC) and database client protocols (such as the MySQL client/server protocol and the PostgreSQL frontend/backend protocol) were designed. But as networks have become faster and transmission times have dropped, the bottleneck has shifted to the serialization and deserialization steps.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> This is especially true for queries that produce the larger result sizes characteristic of many data engineering and data analytics pipelines.</p>

<p>Yet many query results today continue to flow through legacy APIs and protocols that add massive serialization and deserialization (“ser/de”) overheads by forcing data into inefficient transfer formats. In a <a href="https://www.vldb.org/pvldb/vol14/p534-li.pdf" target="_blank">2021 paper</a>, Tianyu Li et al. presented an example using ODBC and the PostgreSQL protocol in which 99.996% of total query time was spent on ser/de. That is arguably an extreme case, but we have observed 90% or higher in many real-world cases. Today, for data engineering and data analytics queries, there is a strong incentive to choose a transfer format that speeds up ser/de.</p>

<p>Enter Arrow.</p>

<p>The Apache Arrow open source project defines a <a href="https://arrow.apache.org/docs/format/Columnar.html" target="_blank">data format</a> that is designed to speed up—and in many cases eliminate—ser/de in query result transfer. Since its creation in 2016, the Arrow format and the multi-language toolbox built around it have gained widespread use, but the technical details of how Arrow is able to slash ser/de overheads remain poorly understood. To help address this, we outline five key attributes of the Arrow format that make this possible.</p>

<h3 id="1-the-arrow-format-is-columnar">1. The Arrow format is columnar.</h3>

<p>Columnar (column-oriented) data formats hold the values for each column in contiguous blocks of memory. This is in contrast to row-oriented data formats, which hold the values for each row in contiguous blocks of memory.</p>

<figure style="text-align: center;">
  <img src="/img/arrow-result-transfer/part-1-figure-1-row-vs-column-layout.png" width="100%" class="img-responsive" alt="Figure 1: An illustration of row-oriented and column-oriented physical memory layouts of a table containing three rows and five columns." />
  <figcaption>Figure 1: An illustration of row-oriented and column-oriented physical memory layouts of a table containing three rows and five columns.</figcaption>
</figure>

<p>High-performance analytic databases, data warehouses, query engines, and storage systems have converged on columnar architecture because it speeds up the most common types of analytic queries. Examples of modern columnar query systems include Amazon Redshift, Apache DataFusion, ClickHouse, Databricks Photon Engine, DuckDB, Google BigQuery, Microsoft Azure Synapse Analytics, OpenText Analytics Database (Vertica), Snowflake, and Voltron Data Theseus.</p>

<p>Likewise, many destinations for analytic query results (such as business intelligence tools, data application platforms, dataframe libraries, and machine learning platforms) use columnar architecture. Examples of columnar business intelligence tools include Amazon QuickSight, Domo, GoodData, Power BI, Qlik Sense, Spotfire, and Tableau. Examples of columnar dataframe libraries include cuDF, pandas, and Polars.</p>

<p>So it is increasingly common for both the source format and the target format of a query result to be columnar formats. The most efficient way to transfer data between a columnar source and a columnar target is to use a columnar transfer format. This eliminates the need for a time-consuming transpose of the data from columns to rows at the source during the serialization step and another time-consuming transpose of the data from rows to columns at the destination during the deserialization step.</p>

<p>Arrow is a columnar data format. The column-oriented layout of data in the Arrow format is similar—and in many cases identical—to the layout of data in many widely used columnar source systems and destination systems.</p>

<h3 id="2-the-arrow-format-is-self-describing-and-type-safe">2. The Arrow format is self-describing and type-safe.</h3>

<p>In a self-describing data format, the schema (the names and types of the columns) and other metadata that describe the data’s structure are included with the data. A self-describing format provides the receiving system with all the information it needs to safely and efficiently process the data. By contrast, when a format is not self-describing, the receiving system must scan the data to infer its schema and structure (a slow and error-prone process) or obtain the schema separately.</p>

<p>An important property of some self-describing data formats is the ability to enforce type safety. When a format enforces type safety, it guarantees that data values conform to their specified types, thereby allowing the receiving system to rule out the possibility of type errors when processing the data. By contrast, when a format does not enforce type safety, the receiving system must check the validity of each individual value in the data (a computationally expensive process) or handle type errors when processing the data.</p>

<p>When reading data from a non-self-describing, type-unsafe format (such as CSV), all this scanning, inferring, and checking contributes to large deserialization overheads. Worse, such formats can lead to ambiguities, debugging trouble, maintenance challenges, and security vulnerabilities.</p>

<p>The Arrow format is self-describing and enforces type safety. Furthermore, Arrow’s type system is similar to—and in many cases identical to or a superset of—the type systems of many widely used data sources and destinations. This includes most columnar data systems and many row-oriented systems such as Apache Spark and various relational databases. When using the Arrow format, these systems can quickly and safely convert data values between their native types and the corresponding Arrow types.</p>

<h3 id="3-the-arrow-format-enables-zero-copy">3. The Arrow format enables zero-copy.</h3>

<p>A zero-copy operation is one in which data is transferred from one medium to another without creating any intermediate copies. When a data format supports zero-copy operations, this implies that its structure in memory is the same as its structure on disk or on the network. So, for example, the data can be read off the network directly into a usable structure in memory without performing any intermediate copies or conversions.</p>

<p>The Arrow format supports zero-copy operations. To hold sets of data values, Arrow defines a column-oriented tabular data structure called a <a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc" target="_blank">record batch</a>. Arrow record batches can be held in memory, sent over a network, or stored on disk. The binary structure remains the same regardless of which medium a record batch is on and which system generated it. To hold schemas and other metadata, Arrow internally uses FlatBuffers, a format created by Google which also has the same binary structure regardless of which medium it is on.</p>

<p>As a result of these design choices, Arrow can serve not only as a transfer format but also as an in-memory format and on-disk format. This is in contrast to text-based formats such as JSON and CSV and serialized binary formats such as Protocol Buffers and Thrift, which encode data values using dedicated structural syntax. To load data from these formats into a usable in-memory structure, the data must be parsed and decoded. This is also in contrast to binary formats such as Parquet and ORC, which use encodings and compression to reduce the size of the data on disk. To load data from these formats into a usable in-memory structure, it must be decompressed and decoded.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p>

<p>This means that at the source system, if data exists in memory or on disk in Arrow format, that data can be transmitted over the network in Arrow format without any serialization. And at the destination system, Arrow-formatted data can be read off the network into memory or into Arrow files on disk without any deserialization.</p>

<p>The Arrow format was designed to be highly efficient as an in-memory format for analytic operations. Because of this, many columnar data systems have been built using Arrow as their in-memory format. These include Apache DataFusion, cuDF, Dremio, InfluxDB, Polars, Velox, and Voltron Data Theseus. When one of these systems is the source or destination of a transfer, ser/de overheads can be fully eliminated. With most other columnar data systems, the proprietary in-memory formats they use are very similar to Arrow. With those systems, serialization to Arrow and deserialization from Arrow format are fast and efficient.</p>

<h3 id="4-the-arrow-format-enables-streaming">4. The Arrow format enables streaming.</h3>

<p>A streamable data format is one that can be processed sequentially, one chunk at a time, without waiting for the full dataset. When data is being transmitted in a streamable format, the receiving system can begin processing it as soon as the first chunk arrives. This can speed up data transfer in several ways: transfer time can overlap with processing time; the receiving system can use memory more efficiently; and multiple streams can be transferred in parallel, speeding up transmission, deserialization, and processing.</p>

<p>CSV is an example of a streamable data format, because the column names (if included) are in a header at the top of the file, and the lines in the file can be processed sequentially. Parquet and ORC are examples of data formats that do not enable streaming, because the schema and other metadata, which are required to process the data, are held in a footer at the bottom of the file, making it necessary to download the entire file (or seek to the end of the file and download the footer separately) before any processing can begin.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p>

<p>Arrow is a streamable data format. A dataset can be represented in Arrow as a sequence of record batches that all have the same schema. Arrow defines a <a href="https://arrow.apache.org/docs/format/Columnar.html#ipc-streaming-format" target="_blank">streaming format</a> consisting of the schema followed by one or more record batches. A system receiving an Arrow stream can process the record batches sequentially as they arrive.</p>

<figure style="text-align: center;">
  <img src="/img/arrow-result-transfer/part-1-figure-2-arrow-stream.png" width="100%" class="img-responsive" alt="Figure 2: An illustration of an Arrow stream transmitting data from a table with three columns. The first record batch contains the values for the first three rows, the second record batch contains the values for the next three rows, and so on. Actual Arrow record batches might contain thousands to millions of rows." />
  <figcaption>Figure 2: An illustration of an Arrow stream transmitting data from a table with three columns. The first record batch contains the values for the first three rows, the second record batch contains the values for the next three rows, and so on. Actual Arrow record batches might contain thousands to millions of rows.</figcaption>
</figure>

<h3 id="5-the-arrow-format-is-universal">5. The Arrow format is universal.</h3>

<p>Arrow has emerged as a de facto standard format for working with tabular data in memory. The Arrow format is a language-independent open standard. Libraries are available for working with Arrow data in languages including C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, R, Ruby, Rust, and Swift. Applications developed in virtually any mainstream language can add support for sending or receiving data in Arrow format. Data does not need to pass through a specific language runtime, like it must with some database connectivity APIs, including JDBC.</p>

<p>Arrow’s universality allows it to address a fundamental problem in speeding up real-world data systems: Performance improvements are inherently constrained by a system’s bottlenecks. This problem is known as <a href="https://www.geeksforgeeks.org/computer-organization-amdahls-law-and-its-proof/" target="_blank">Amdahl’s law</a>. In real-world data pipelines, query results often flow through multiple stages, incurring ser/de overheads at each stage. If, for example, your data pipeline has five stages and you eliminate ser/de overheads in four of them, your system might be no faster than before because ser/de in the one remaining stage will bottleneck the full pipeline.</p>

<p>Arrow’s ability to operate efficiently in virtually any technology stack helps to solve this problem. Does your data flow from a Scala-based distributed backend with NVIDIA GPU-accelerated workers to a Jetty-based HTTP server then to a Rails-powered feature engineering app which users interact with through a Node.js-based machine learning framework with a Pyodide-based browser front end? No problem; Arrow libraries are available to eliminate ser/de overheads between all of those components.</p>

<h3 id="conclusion">Conclusion</h3>

<p>As more commercial and open source tools have added support for Arrow, fast query result transfer with low or no ser/de overheads has become increasingly common. Today, commercial data platforms and query engines including Databricks, Dremio, Google BigQuery, InfluxDB, Snowflake, and Voltron Data Theseus and open source databases and query engines including Apache DataFusion, Apache Doris, Apache Spark, ClickHouse, and DuckDB can all transfer query results in Arrow format. The speedups are substantial:</p>

<ul>
  <li>Apache Doris: <a href="https://doris.apache.org/blog/arrow-flight-sql-in-apache-doris-for-10x-faster-data-transfer" target="_blank">faster “by a factor ranging from 20 to several hundreds”</a></li>
  <li>Google BigQuery: <a href="https://medium.com/google-cloud/announcing-google-cloud-bigquery-version-1-17-0-1fc428512171" target="_blank">up to “31x faster”</a></li>
  <li>Dremio: <a href="https://www.dremio.com/press-releases/dremio-announces-support-for-apache-arrow-flight-high-performance-data-transfer/" target="_blank">“more than 10 times faster”</a></li>
  <li>DuckDB: <a href="https://duckdb.org/2023/08/04/adbc.html#benchmark-adbc-vs-odbc" target="_blank">“38x” faster</a></li>
  <li>Snowflake: <a href="https://www.snowflake.com/en/blog/fetching-query-results-from-snowflake-just-got-a-lot-faster-with-apache-arrow/" target="_blank">“up to a 10x” faster</a></li>
</ul>

<p>On the receiving side, data practitioners can maximize speedups by using Arrow-based tools and Arrow libraries, interfaces, and protocols. In 2025, as more projects and vendors implement support for the <a href="https://arrow.apache.org/adbc/" target="_blank">ADBC</a> standard, we expect to see accelerating growth in the number of tools that can receive query results in Arrow format.</p>

<p>Stay tuned for upcoming posts in this series, which will compare the Arrow format to other data formats and describe the protocols and APIs that clients can use to fetch results in Arrow format.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>The transfer format may also be called the wire format or serialization format. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>From the 1990s to today, increases in network performance outpaced increases in CPU performance. For example, in the late 1990s, a mainstream desktop CPU could perform roughly 1 GFLOPS and a typical WAN connection speed was 56 Kb/s. Today, a mainstream desktop CPU can perform roughly 100 GFLOPS and WAN connection speeds of around 1 Gb/s are common. So while CPU performance increased by about 100x, network speed increased by about 10,000x. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>This does not imply that Arrow is faster than Parquet or ORC in other applications such as archival storage. An upcoming post in this series will compare the Arrow format to these and other formats in more technical detail and describe how they often complement each other. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>This does not imply that CSV will transfer results faster than Parquet or ORC. When comparing the transfer performance of CSV to Parquet or ORC, the other attributes described here will typically outweigh this one. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ian Cook, David Li, Matt Topol</name></author><category term="application" /><summary type="html"><![CDATA[Arrow speeds up query result transfer by slashing (de)serialization overheads. We outline five key attributes of the Arrow format that enable this.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-result-transfer/part-1-share-image.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-result-transfer/part-1-share-image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 18.1.0 Release</title><link href="https://arrow.apache.org/blog/2024/11/21/18.1.0-release/" rel="alternate" type="text/html" title="Apache Arrow 18.1.0 Release" /><published>2024-11-21T00:00:00-05:00</published><updated>2024-11-21T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2024/11/21/18.1.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2024/11/21/18.1.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 18.1.0 release. This
covers roughly one month of development work and includes <a href="https://github.com/apache/arrow/milestone/67?closed=1"><strong>15 resolved
issues</strong></a> on <a href="/release/18.1.0.html#contributors"><strong>25 distinct commits</strong></a> from <a href="/release/18.1.0.html#contributors"><strong>8 distinct
contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a> to
learn how to get the libraries for your platform.</p>

<p>The primary motivation for this minor release was to address packaging issues
for Python, Java, C#, R and MATLAB which caused binaries for those platforms to
report a version of <code class="language-plaintext highlighter-rouge">18.0.0-SNAPSHOT</code> instead of <code class="language-plaintext highlighter-rouge">18.0.0</code>. The affected binaries
were otherwise built correctly. See <a href="https://lists.apache.org/thread/t8k7l2hsbgdt7cszj7hrpjdfpn91n5zb">this mailing list
post</a> for more
details.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Numerous other bugfixes and improvements have been made:
we refer you to the <a href="/release/18.1.0.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Moved <code class="language-plaintext highlighter-rouge">ChunkResolver</code> into the public API. <a href="https://github.com/apache/arrow/issues/34535">GH-34535</a></li>
</ul>

<h2 id="c-notes-1">C# notes</h2>

<ul>
  <li>Fixed <a href="https://github.com/apache/arrow/issues/44360">GH-44360</a> - [C#] Flight DoExchange server is incompatible with C++/PyArrow client</li>
</ul>

<h2 id="rust-and-go-notes">Rust and Go notes</h2>

<p>The Rust and Go projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.
For notes on the latest release of the Go implementation, see the latest
<a href="https://github.com/apache/arrow-go/tags">Arrow Go changelog</a></p>

<h2 id="linux-packages-notes">Linux packages notes</h2>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 18.1.0 release. This covers roughly one month of development work and includes 15 resolved issues on 25 distinct commits from 8 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The primary motivation for this minor release was to address packaging issues for Python, Java, C#, R and MATLAB which caused binaries for those platforms to report a version of 18.0.0-SNAPSHOT instead of 18.0.0. The affected binaries were otherwise built correctly. See this mailing list post for more details. The release notes below are not exhaustive and only expose selected highlights of the release. Numerous other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes Moved ChunkResolver into the public API. GH-34535 C# notes Fixed GH-44360 - [C#] Flight DoExchange server is incompatible with C++/PyArrow client Rust and Go notes The Rust and Go projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog Linux packages notes]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 15 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2024/11/13/adbc-15-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 15 (Libraries) Release" /><published>2024-11-13T00:00:00-05:00</published><updated>2024-11-13T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2024/11/13/adbc-15-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2024/11/13/adbc-15-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the version 15 release 
of the Apache Arrow ADBC libraries. This release includes <a href="https://github.com/apache/arrow-adbc/milestone/19"><strong>31
resolved issues</strong></a> from <a href="#contributors"><strong>13 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version</p>
<ol>
  <li>The <a href="https://arrow.apache.org/adbc/15/format/specification.html"><strong>API specification</strong></a> is versioned 
separately and is at version 1.1.0.</li>
</ol>

<p>The subcomponents are versioned independently:</p>

<ul>
  <li>C/C++/GLib/Go/Python/Ruby: 1.3.0</li>
  <li>C#: 0.15.0</li>
  <li>Java: 0.15.0</li>
  <li>R: 0.15.0</li>
  <li>Rust: 0.15.0</li>
</ul>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-15/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<ul>
  <li>The BigQuery driver is now properly released to PyPI. See 
<a href="https://pypi.org/project/adbc-driver-bigquery/">adbc-driver-bigquery</a>.```</li>
  <li>A basic driver for Apache DataFusion is now being developed.</li>
  <li>The documentation now includes the Doxygen API reference for C/C++, which should give a better/more native experience than the previous Breathe-based documentation.</li>
  <li>The Java libraries now use the latest arrow-java libraries, and as such require Java 11 or newer.</li>
  <li>The PostgreSQL driver has basic support for Redshift, though it cannot use the COPY optimizations for PostgreSQL and as such will not be as fast.</li>
  <li>The PostgreSQL driver can now handle ingesting Arrow list types.</li>
  <li>The PostgreSQL driver will use the <a href="https://arrow.apache.org/docs/format/CanonicalExtensions.html#opaque">Opaque canonical extension type</a> for unknown types, instead of just returning bytes with no further context.</li>
  <li>We no longer build for Python 3.8.  We now build for Python 3.13.</li>
  <li>The Snowflake driver better handles catalog operations when not connected to a particular database.</li>
</ul>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-14..apache-arrow-adbc-15
    24	David Li
    15	Dewey Dunnington
    14	Bruce Irschick
     5	Curt Hagenlocher
     5	davidhcoe
     3	Laurent Goujon
     3	Matthijs Brobbel
     3	William Ayd
     3	eitsupi
     2	Matt Topol
     2	Tornike Gurgenidze
     2	qifanzhang-ms
     1	Sudhir Reddy Emmadi
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support.  For more, see the <a href="https://github.com/apache/arrow-adbc/milestone/8">milestone</a>; the proposed C Data Interface extensions have been accepted.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the version 15 release of the Apache Arrow ADBC libraries. This release includes 31 resolved issues from 13 distinct contributors. This is a release of the libraries, which are at version The API specification is versioned separately and is at version 1.1.0. The subcomponents are versioned independently: C/C++/GLib/Go/Python/Ruby: 1.3.0 C#: 0.15.0 Java: 0.15.0 R: 0.15.0 Rust: 0.15.0 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights The BigQuery driver is now properly released to PyPI. See adbc-driver-bigquery.``` A basic driver for Apache DataFusion is now being developed. The documentation now includes the Doxygen API reference for C/C++, which should give a better/more native experience than the previous Breathe-based documentation. The Java libraries now use the latest arrow-java libraries, and as such require Java 11 or newer. The PostgreSQL driver has basic support for Redshift, though it cannot use the COPY optimizations for PostgreSQL and as such will not be as fast. The PostgreSQL driver can now handle ingesting Arrow list types. The PostgreSQL driver will use the Opaque canonical extension type for unknown types, instead of just returning bytes with no further context. We no longer build for Python 3.8. We now build for Python 3.13. The Snowflake driver better handles catalog operations when not connected to a particular database. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-14..apache-arrow-adbc-15 24 David Li 15 Dewey Dunnington 14 Bruce Irschick 5 Curt Hagenlocher 5 davidhcoe 3 Laurent Goujon 3 Matthijs Brobbel 3 William Ayd 3 eitsupi 2 Matt Topol 2 Tornike Gurgenidze 2 qifanzhang-ms 1 Sudhir Reddy Emmadi Roadmap There is some discussion on a potential second revision of ADBC to include more missing functionality and asynchronous API support. For more, see the milestone; the proposed C Data Interface extensions have been accepted. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 18.0.0 Release</title><link href="https://arrow.apache.org/blog/2024/10/28/18.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 18.0.0 Release" /><published>2024-10-28T00:00:00-04:00</published><updated>2024-10-28T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2024/10/28/18.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2024/10/28/18.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 18.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/64?closed=1"><strong>334 resolved issues</strong></a>
on <a href="/release/18.0.0.html#contributors"><strong>530 distinct commits</strong></a> from <a href="/release/18.0.0.html#contributors"><strong>89 distinct contributors</strong></a>.
See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/18.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 17.0.0 release, Will Ayd and Rossi Sun have been invited to be committer.
No new members have joined the Project Management Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format">Columnar format</h2>

<p>The Arrow columnar format now allows 32-bit and 64-bit decimal data, in
addition to the already existing 128-bit and 256-bit decimal data types
(GH-43956).</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>The Java implementation now transparently handles compressed Arrow data when reading, instead of requiring explicit configuration. (GH-43469)
The Ruby bindings now support implementing DoPut on the server. (GH-43814)</p>

<h2 id="c-notes">C++ notes</h2>

<p>The default memory pool has changed to mimalloc on all platforms (GH-43254).
Previously, jemalloc was used by default on Linux. Using mimalloc by default
provides a more consistent experience across different platforms, and
makes configuration easier. It is expected that this might either increase
or decrease performance on user workloads that use the default memory pool;
please benchmark accordingly. Jemalloc can still be selected by setting
the <a href="https://arrow.apache.org/docs/cpp/env_vars.html#envvar-ARROW_DEFAULT_MEMORY_POOL"><code class="language-plaintext highlighter-rouge">ARROW_DEFAULT_MEMORY_POOL</code></a> environment variable to “jemalloc”.</p>

<p>A new class <code class="language-plaintext highlighter-rouge">arrow::ArrayStatistics</code> has been added to encode basic statistics
about an Arrow array. It provides a source-agnostic representation for statistics
provided by third-party sources such as Parquet files (GH-41909).</p>

<p>The new Decimal32 and Decimal64 types have been made available (GH-43956).</p>

<p>Several canonical extension types have been implemented:</p>
<ul>
  <li>the <a href="https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#opaque">Opaque</a> extension type (GH-43454);</li>
  <li>the <a href="https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#bit-boolean">8-bit boolean</a> extension type (GH-17682);</li>
  <li>the <a href="https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#uuid">UUID</a> extension type (GH-15058);</li>
  <li>the <a href="https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#json">JSON</a> extension type (GH-32538).</li>
</ul>

<p><strong>Flight UCX is deprecated.</strong> We plan to remove this experiment in the next couple of releases.</p>

<h3 id="acero">Acero</h3>

<ul>
  <li>Enhanced the row-oriented representation by widening the offset type from 32-bit to 64-bit, resolving crashes and data corruption in aggregation and hash join on large datasets due to offset overflow (GH-43495).</li>
  <li>Improved ordered aggregation performance by reducing complexity from <code class="language-plaintext highlighter-rouge">O(n*m)</code> to <code class="language-plaintext highlighter-rouge">O(n)</code>, where <code class="language-plaintext highlighter-rouge">n</code> is the number of rows and <code class="language-plaintext highlighter-rouge">m</code> the number of segments in the batch (GH-44052).</li>
</ul>

<h3 id="compute">Compute</h3>

<p>Casting between string-like and string-view-like types has been implemented (GH-42247).</p>

<h3 id="dataset">Dataset</h3>

<h3 id="filesystems">Filesystems</h3>

<p>Writing small files to S3 can use a single S3 API call instead of three,
provided the new option <code class="language-plaintext highlighter-rouge">allow_delayed_open</code> is enabled (GH-40557).
Files larger than 5 MB still go through the regular multipart
upload mechanism.</p>

<p>Background writes are now implemented and enabled by default for the Azure
filesystem, dramatically improving the performance of writing to remote files
(GH-40036).</p>

<p>Finalization of the S3 filesystem layer should hopefully be more robust (GH-44071).</p>

<h3 id="gandiva">Gandiva</h3>

<p>LLVM 19.1 is now supported (GH-44222).</p>

<h3 id="gpu">GPU</h3>

<h3 id="ipc">IPC</h3>

<p>The seed corpus used for fuzzing the IPC reader has been improved, hopefully
helping make the IPC reader even more robust against corrupt or malicious
IPC streams (GH-38041).</p>

<h3 id="parquet">Parquet</h3>

<p>A new command line utility <code class="language-plaintext highlighter-rouge">parquet-dump-footer</code> allows dumping the Thrift-encoded
footer metadata of a Parquet file, optionally scrubbing confidential data
(GH-42102). This is part of the effort to collect real-world Parquet metadata
so as to evaluate the efficiency of future improvements to the Parquet format.
Please see https://github.com/apache/parquet-benchmark for instructions to submit
footers representative of your own workloads.</p>

<h2 id="c-notes-1">C# notes</h2>

<ul>
  <li>Partial support has been added for LargeBinary, LargeString and LargeList. The column sizes cannot exceed 2 GB in length. (GH-43266).</li>
  <li>Changes to Flight support were made for better control and compatibility, and to allow Flight Server to be hosted in pre-Kestrel versions of .NET (GH-43907, GH-43672, GH-41347).</li>
  <li>Support has been added for newly-defined types decimal32 and decimal64 (GH-44271).</li>
  <li>The import of sliced arrays through the C Data interface now works correctly. (GH-43267)
    <h2 id="java-notes">Java notes</h2>
  </li>
</ul>

<p><strong>Java 8 is no longer supported.</strong> (GH-38051)</p>

<p><strong>Gandiva may not work in this release.</strong> For details, please see <a href="https://github.com/apache/arrow/issues/43576">GH-43576</a>.</p>

<p>Basic support for RunEndEncoded was added (GH-39982). The ListView/StringView vector implementations are now more complete, including C Data support (multiple issues).</p>

<p>Several APIs have been updated to accept <code class="language-plaintext highlighter-rouge">long</code> for addresses in preparation for FFM/large buffer support (GH-43902). We no longer expose <code class="language-plaintext highlighter-rouge">sun.misc.Unsafe</code> (GH-43479). We no longer ship the <code class="language-plaintext highlighter-rouge">shaded</code> flight-core JARs (GH-43217).</p>

<p>More options were added to the Dataset ScanOptions API (GH-28866).</p>

<h2 id="javascript-notes">JavaScript notes</h2>
<ul>
  <li>Accessing individual rows in Tables or Structs should now be more performant (<a href="https://github.com/apache/arrow/issues/30863">GH-30863</a>).</li>
</ul>

<h2 id="python-notes">Python notes</h2>
<p>Compatibility notes:</p>
<ul>
  <li>NumPy required dependency has been removed from pyarrow packaging
<a href="https://github.com/apache/arrow/issues/43846">GH-43846</a> and has been
made an optional runtime dependency <a href="https://github.com/apache/arrow/issues/25118">GH-25118</a>.</li>
  <li>Support for Python 3.8 has been dropped <a href="https://github.com/apache/arrow/issues/43518">GH-43518</a></li>
  <li>No longer used serialize/deserialize Pyarrow C++ functions have been
deprecated <a href="https://github.com/apache/arrow/issues/44063">GH-44063</a>.</li>
  <li>Passing of build flags to setup.py (e.g. <code class="language-plaintext highlighter-rouge">setup.py --with-parquet</code>) has been
deprecated <a href="https://github.com/apache/arrow/issues/43514">GH-43514</a></li>
</ul>

<p>New features:</p>
<ul>
  <li>Non-cpu work has continued with <a href="https://github.com/apache/arrow/issues/43973">GH-43973</a>,
<a href="https://github.com/apache/arrow/issues/43728">GH-43728</a>, <a href="https://github.com/apache/arrow/issues/43727">GH-43727</a>,
<a href="https://github.com/apache/arrow/issues/43391">GH-43391</a>,
<a href="https://github.com/apache/arrow/issues/42222">GH-42222</a> and
<a href="https://github.com/apache/arrow/issues/41665">GH-41665</a>.</li>
  <li>Arrow C++ <code class="language-plaintext highlighter-rouge">arrow::dataset::Partitioning::Format</code> method has been exposed in
Python <a href="https://github.com/apache/arrow/issues/43684">GH-43684</a>.</li>
  <li>UUID canonical extension type is now supported in Python
<a href="https://github.com/apache/arrow/issues/15058">GH-15058</a>.</li>
  <li>Opaque canonical extension type has been implemented
<a href="https://github.com/apache/arrow/issues/43454">GH-43454</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">StructArray.from_array</code> now accepts a type in addition to names or fields
<a href="https://github.com/apache/arrow/issues/42014">GH-42014</a>.</li>
  <li>New attributes have been added to <code class="language-plaintext highlighter-rouge">StructType</code> in order to access all its fields
<a href="https://github.com/apache/arrow/issues/30058">GH-30058</a>.</li>
</ul>

<p>Other improvements:</p>
<ul>
  <li>In order to support free-threaded build of CPython 3.13 additional work has been made:
<a href="https://github.com/apache/arrow/issues/44046">GH-44046</a>,
<a href="https://github.com/apache/arrow/issues/44355">GH-44355</a> and
<a href="https://github.com/apache/arrow/issues/43964">GH-43964</a>. Umbrella issue
<a href="https://github.com/apache/arrow/issues/43536">GH-43536</a>.</li>
  <li>PyCapsule interface now has precedence over others in pa.schema(..)
<a href="https://github.com/apache/arrow/issues/43388">GH-43388</a>.</li>
  <li>Usage of deprecated <code class="language-plaintext highlighter-rouge">pkg_resources</code> in setup.py has been replaced with
<code class="language-plaintext highlighter-rouge">numpy.get_include()</code> <a href="https://github.com/apache/arrow/issues/43532">GH-43532</a>.</li>
  <li>Conversion from Arrow to JAX via dlpack as added to the documentation examples
<a href="https://github.com/apache/arrow/issues/44229">GH-44229</a>.</li>
</ul>

<p>Relevant bug fixes:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.Table.rename_columns</code> has been updated and should have accepted <code class="language-plaintext highlighter-rouge">tuples</code>,
not only <code class="language-plaintext highlighter-rouge">list</code> or <code class="language-plaintext highlighter-rouge">dict</code>. This has been fixed
<a href="https://github.com/apache/arrow/issues/43588">GH-43588</a>.</li>
  <li>Python reference handling in UDF implementation has been sanitized
<a href="https://github.com/apache/arrow/issues/43487">GH-43487</a>.</li>
  <li>Files included when building wheels have been cleaned (unnecessary files removed)
<a href="https://github.com/apache/arrow/issues/43299">GH-43299</a>.</li>
</ul>

<h2 id="r-notes">R notes</h2>

<p>For more on what’s in the 18.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Add workaround for install failure due to <code class="language-plaintext highlighter-rouge">re2.pc</code> on Ubuntu 20.04: <a href="https://github.com/apache/arrow/issues/41396">GH-41396</a></li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">0</code> decimal value: <a href="https://github.com/apache/arrow/issues/43877">GH-43877</a></li>
</ul>

<p>C GLib related improvements are also available in Ruby.</p>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Add support for Azure file system: <a href="https://github.com/apache/arrow/issues/43738">GH-43738</a></li>
  <li>FlightRPC: Add support for DoPut: <a href="https://github.com/apache/arrow/issues/41056">GH-41056</a></li>
  <li>FlightRPC: Add support for timeout: <a href="https://github.com/apache/arrow/issues/44178">GH-44178</a></li>
  <li>Parquet: Add support for writing a record batch: <a href="https://github.com/apache/arrow/issues/40860">GH-40860</a></li>
  <li>Add support for pull style IPC stream format decoder: <a href="https://github.com/apache/arrow/issues/40493">GH-40493</a></li>
</ul>

<h2 id="rust-notes-and-go-notes">Rust notes and Go notes</h2>

<p>The Rust and Go projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.
For notes on the latest release of the Go implementation, see the latest
<a href="https://github.com/apache/arrow-go/tags">Arrow Go changelog</a></p>

<h2 id="linux-packages-notes">Linux packages notes</h2>

<p>The Azure file system is now enabled.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 18.0.0 release. This covers over 3 months of development work and includes 334 resolved issues on 530 distinct commits from 89 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 17.0.0 release, Will Ayd and Rossi Sun have been invited to be committer. No new members have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Columnar format The Arrow columnar format now allows 32-bit and 64-bit decimal data, in addition to the already existing 128-bit and 256-bit decimal data types (GH-43956). Arrow Flight RPC notes The Java implementation now transparently handles compressed Arrow data when reading, instead of requiring explicit configuration. (GH-43469) The Ruby bindings now support implementing DoPut on the server. (GH-43814) C++ notes The default memory pool has changed to mimalloc on all platforms (GH-43254). Previously, jemalloc was used by default on Linux. Using mimalloc by default provides a more consistent experience across different platforms, and makes configuration easier. It is expected that this might either increase or decrease performance on user workloads that use the default memory pool; please benchmark accordingly. Jemalloc can still be selected by setting the ARROW_DEFAULT_MEMORY_POOL environment variable to “jemalloc”. A new class arrow::ArrayStatistics has been added to encode basic statistics about an Arrow array. It provides a source-agnostic representation for statistics provided by third-party sources such as Parquet files (GH-41909). The new Decimal32 and Decimal64 types have been made available (GH-43956). Several canonical extension types have been implemented: the Opaque extension type (GH-43454); the 8-bit boolean extension type (GH-17682); the UUID extension type (GH-15058); the JSON extension type (GH-32538). Flight UCX is deprecated. We plan to remove this experiment in the next couple of releases. Acero Enhanced the row-oriented representation by widening the offset type from 32-bit to 64-bit, resolving crashes and data corruption in aggregation and hash join on large datasets due to offset overflow (GH-43495). Improved ordered aggregation performance by reducing complexity from O(n*m) to O(n), where n is the number of rows and m the number of segments in the batch (GH-44052). Compute Casting between string-like and string-view-like types has been implemented (GH-42247). Dataset Filesystems Writing small files to S3 can use a single S3 API call instead of three, provided the new option allow_delayed_open is enabled (GH-40557). Files larger than 5 MB still go through the regular multipart upload mechanism. Background writes are now implemented and enabled by default for the Azure filesystem, dramatically improving the performance of writing to remote files (GH-40036). Finalization of the S3 filesystem layer should hopefully be more robust (GH-44071). Gandiva LLVM 19.1 is now supported (GH-44222). GPU IPC The seed corpus used for fuzzing the IPC reader has been improved, hopefully helping make the IPC reader even more robust against corrupt or malicious IPC streams (GH-38041). Parquet A new command line utility parquet-dump-footer allows dumping the Thrift-encoded footer metadata of a Parquet file, optionally scrubbing confidential data (GH-42102). This is part of the effort to collect real-world Parquet metadata so as to evaluate the efficiency of future improvements to the Parquet format. Please see https://github.com/apache/parquet-benchmark for instructions to submit footers representative of your own workloads. C# notes Partial support has been added for LargeBinary, LargeString and LargeList. The column sizes cannot exceed 2 GB in length. (GH-43266). Changes to Flight support were made for better control and compatibility, and to allow Flight Server to be hosted in pre-Kestrel versions of .NET (GH-43907, GH-43672, GH-41347). Support has been added for newly-defined types decimal32 and decimal64 (GH-44271). The import of sliced arrays through the C Data interface now works correctly. (GH-43267) Java notes Java 8 is no longer supported. (GH-38051) Gandiva may not work in this release. For details, please see GH-43576. Basic support for RunEndEncoded was added (GH-39982). The ListView/StringView vector implementations are now more complete, including C Data support (multiple issues). Several APIs have been updated to accept long for addresses in preparation for FFM/large buffer support (GH-43902). We no longer expose sun.misc.Unsafe (GH-43479). We no longer ship the shaded flight-core JARs (GH-43217). More options were added to the Dataset ScanOptions API (GH-28866). JavaScript notes Accessing individual rows in Tables or Structs should now be more performant (GH-30863). Python notes Compatibility notes: NumPy required dependency has been removed from pyarrow packaging GH-43846 and has been made an optional runtime dependency GH-25118. Support for Python 3.8 has been dropped GH-43518 No longer used serialize/deserialize Pyarrow C++ functions have been deprecated GH-44063. Passing of build flags to setup.py (e.g. setup.py --with-parquet) has been deprecated GH-43514 New features: Non-cpu work has continued with GH-43973, GH-43728, GH-43727, GH-43391, GH-42222 and GH-41665. Arrow C++ arrow::dataset::Partitioning::Format method has been exposed in Python GH-43684. UUID canonical extension type is now supported in Python GH-15058. Opaque canonical extension type has been implemented GH-43454. StructArray.from_array now accepts a type in addition to names or fields GH-42014. New attributes have been added to StructType in order to access all its fields GH-30058. Other improvements: In order to support free-threaded build of CPython 3.13 additional work has been made: GH-44046, GH-44355 and GH-43964. Umbrella issue GH-43536. PyCapsule interface now has precedence over others in pa.schema(..) GH-43388. Usage of deprecated pkg_resources in setup.py has been replaced with numpy.get_include() GH-43532. Conversion from Arrow to JAX via dlpack as added to the documentation examples GH-44229. Relevant bug fixes: pyarrow.Table.rename_columns has been updated and should have accepted tuples, not only list or dict. This has been fixed GH-43588. Python reference handling in UDF implementation has been sanitized GH-43487. Files included when building wheels have been cleaned (unnecessary files removed) GH-43299. R notes For more on what’s in the 18.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Add workaround for install failure due to re2.pc on Ubuntu 20.04: GH-41396 Add support for 0 decimal value: GH-43877 C GLib related improvements are also available in Ruby. C GLib Add support for Azure file system: GH-43738 FlightRPC: Add support for DoPut: GH-41056 FlightRPC: Add support for timeout: GH-44178 Parquet: Add support for writing a record batch: GH-40860 Add support for pull style IPC stream format decoder: GH-40493 Rust notes and Go notes The Rust and Go projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog. For notes on the latest release of the Go implementation, see the latest Arrow Go changelog Linux packages notes The Azure file system is now enabled.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Go 18.0.0 Release</title><link href="https://arrow.apache.org/blog/2024/10/23/arrow-go-18.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow Go 18.0.0 Release" /><published>2024-10-23T00:00:00-04:00</published><updated>2024-10-23T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2024/10/23/arrow-go-18.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2024/10/23/arrow-go-18.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the v18.0.0 release of
Apache Arrow Go. This release covers 35 resolved issues from
11 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<h3 id="move-to-new-repository">Move to new Repository</h3>

<p>The Apache Arrow Go implementation has been shifted out of the Arrow monorepo
and to its own repository now located at https://github.com/apache/arrow-go.
This will enable fewer major version releases and facilitate more minor
version/patch releases instead as it will detach the version number from the
Arrow C++ and other implementations in the monorepo.</p>

<p>Current users of Arrow Go will want to carefully note the breaking change to
module paths this move causes. To upgrade to Arrow Go v18.0.0, ensure you
properly update all relevant module paths in your <code class="language-plaintext highlighter-rouge">go.mod</code>, replacing all
instances of <code class="language-plaintext highlighter-rouge">github.com/apache/arrow/go/</code> with <code class="language-plaintext highlighter-rouge">github.com/apache/arrow-go</code>.</p>

<p>Existing Go related issues have been moved to the new repository and please file
any new issues on the new repository instead of the Arrow monorepo.</p>

<h3 id="go-version">Go Version</h3>

<p>With this release, the minimum Go version is now 1.22.</p>

<h3 id="arrow">Arrow</h3>

<h4 id="canonical-extension-types">Canonical Extension Types</h4>

<p>There is a new <a href="https://pkg.go.dev/github.com/apache/arrow-go/v18@v18.0.0/arrow/extensions"><code class="language-plaintext highlighter-rouge">extensions</code></a> package 
which implements <a href="https://arrow.apache.org/docs/format/CanonicalExtensions.html">canonical extension types</a>. This
includes:</p>

<ul>
  <li>Opaque <a href="https://github.com/apache/arrow/issues/43455">GH-43455</a></li>
  <li>JSON and UUID <a href="https://github.com/apache/arrow/issues/43764">GH-43764</a></li>
  <li>Bool8 <a href="https://github.com/apache/arrow/issues/17682">GH-17682</a></li>
</ul>

<h4 id="features">Features</h4>

<ul>
  <li>Added the initial implementation for the Decimal32/Decimal64 data types <a href="https://github.com/apache/arrow-go/issues/120">GH-120</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">is_null</code>, <code class="language-plaintext highlighter-rouge">is_not_null</code> and <code class="language-plaintext highlighter-rouge">is_nan</code> function kernels to the <code class="language-plaintext highlighter-rouge">compute</code> package <a href="https://github.com/apache/arrow-go/issues/134">GH-134</a></li>
</ul>

<h4 id="bug-fixes">Bug Fixes</h4>

<ul>
  <li>Fixed an inconsistency in <code class="language-plaintext highlighter-rouge">ValueOffset</code> between String and Binary Arrays <a href="https://github.com/apache/arrow-go/issues/41">GH-41</a></li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>Added support for LZ4_RAW compression codec <a href="https://github.com/apache/arrow/issues/43790">GH-43790</a></li>
  <li>Fixed recovery from a panic in the file reader <a href="https://github.com/apache/arrow-go/pull/124">GH-124</a></li>
</ul>

<h2 id="contributors">Contributors</h2>

<p>This release consists of contributions from 11 contributors in addition to the
invaluable advice and support of the Apache Arrow community.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">--perl-regexp</span> <span class="nt">--author</span><span class="o">=</span><span class="s1">'^((?!dependabot\[bot\]).*)$'</span> <span class="nt">-sn</span> 8b7fde9..v18.0.0
<span class="go">    17	Sutou Kouhei
     8	Joel Lubinitsky
     6	Matt Topol
     6	Raúl Cumplido
     2	Xin Hao
     1	David Li
     1	Nick Crews
     1	Seb. V
     1	Tom Scott-Coombes
     1	rene-hess
     1	yihao.dai
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the v18.0.0 release of Apache Arrow Go. This release covers 35 resolved issues from 11 contributors. Release Highlights Move to new Repository The Apache Arrow Go implementation has been shifted out of the Arrow monorepo and to its own repository now located at https://github.com/apache/arrow-go. This will enable fewer major version releases and facilitate more minor version/patch releases instead as it will detach the version number from the Arrow C++ and other implementations in the monorepo. Current users of Arrow Go will want to carefully note the breaking change to module paths this move causes. To upgrade to Arrow Go v18.0.0, ensure you properly update all relevant module paths in your go.mod, replacing all instances of github.com/apache/arrow/go/ with github.com/apache/arrow-go. Existing Go related issues have been moved to the new repository and please file any new issues on the new repository instead of the Arrow monorepo. Go Version With this release, the minimum Go version is now 1.22. Arrow Canonical Extension Types There is a new extensions package which implements canonical extension types. This includes: Opaque GH-43455 JSON and UUID GH-43764 Bool8 GH-17682 Features Added the initial implementation for the Decimal32/Decimal64 data types GH-120 Added is_null, is_not_null and is_nan function kernels to the compute package GH-134 Bug Fixes Fixed an inconsistency in ValueOffset between String and Binary Arrays GH-41 Parquet Added support for LZ4_RAW compression codec GH-43790 Fixed recovery from a panic in the file reader GH-124 Contributors This release consists of contributions from 11 contributors in addition to the invaluable advice and support of the Apache Arrow community. $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn 8b7fde9..v18.0.0 17 Sutou Kouhei 8 Joel Lubinitsky 6 Matt Topol 6 Raúl Cumplido 2 Xin Hao 1 David Li 1 Nick Crews 1 Seb. V 1 Tom Scott-Coombes 1 rene-hess 1 yihao.dai]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>